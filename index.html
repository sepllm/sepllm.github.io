<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="DESCRIPTION META TAG">
    <meta property="og:title" content="SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator" />
    <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
    <meta property="og:url" content="URL OF THE WEBSITE" />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/your_banner_image.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />


    <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
    <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>SepLLM</title>
    <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>

<body>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator</h1>
                        <div class="is-size-5 publication-authors">
                            <!-- Paper authors -->
                            <span class="author-block"><a href="https://.github.io/" target="_blank">Guoxuan
                                    Chen</a><sup>1,2</sup>,</span>
                            <span class="author-block"><a href="https://han-shi.github.io/" target="_blank">Han
                                    Shi</a><sup>1,‡</sup>,</span>
                            <span class="author-block"><a href="https://scholar.google.com/citations?user=WRPLXCEAAAAJ" target="_blank">Jiawei
                                    Li</a><sup>1</sup>,</span>
                            <span class="author-block"><a href="https://yihang-gao.github.io/"
                                    target="_blank">Yihang Gao</a><sup>2</sup>,</span>
                            <span class="author-block"><a href="https://scholar.google.com/citations?user=3t2j87YAAAAJ"
                                    target="_blank">Xiaozhe
                                    Ren</a><sup>1</sup>,</span>
                            <br />
                            <span class="author-block"><a href="https://scholar.google.com/citations?user=KalhG8AAAAAJ"
                                    target="_blank">Yimeng
                                    Chen</a><sup>3</sup>,</span>
                            <span class="author-block"><a href="https://scholar.google.com/citations?user=DUfcez0AAAAJ" target="_blank">Xin
                                    Jiang</a><sup>1</sup>,</span>
                            <span class="author-block"><a href="https://scholar.google.com/citations?user=XboZC1AAAAAJ"
                                    target="_blank">Zhenguo
                                    Li</a><sup>1</sup>,</span>
                            <span class="author-block"><a href="https://wyliu.com/" target="_blank">Weiyang
                                    Liu</a><sup>4</sup>,</span>
                            <span class="author-block"><a href="https://sites.google.com/view/chaoh"
                                        target="_blank">Chao
                                        Huang</a><sup>2</sup></span>
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>Huawei Noah's Ark Lab,</span>
                            <span class="author-block"><sup>2</sup>The University of Hong Kong,</span><br />
                            <span class="author-block"><sup>3</sup>Center of Excellence for Generative AI, KAUST</span>
                            <span class="author-block"><sup>4</sup>Max Planck Institute for Intelligent Systems, T&uuml;bingen</span>
                            <span class="eql-cntrb"><small><br><sup>‡</sup>Corresponding
                                    author</small></span>
                            <span class="eql-cntrb"><small><br><a
                                        href="mailto:guoxchen@connect.hku.hk">guoxchen@connect.hku.hk</a>, <a
                                        href="mailto:shi.han@huawei.com">shi.han@huawei.com</a>, <a
                                        href="mailto:li.jiawei@huawei.com">li.jiawei@huawei.com</a>
                                    </small></span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- Github link -->
                                <span class="link-block">
                                    <a href="https://github.com/HKUDS/SepLLM" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>

                                <!-- ArXiv abstract Link -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2309.12284" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>arXiv</span>
                                    </a>
                                </span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <!-- Paper abstract -->
    <section class="section hero is-light">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Large Language Models (LLMs) have exhibited exceptional performance across 
                            a spectrum of natural language processing tasks. However, their substantial 
                            sizes pose considerable challenges, particularly in terms of computational 
                            demands and inference speed, due to its quadratic complexity. In this work, 
                            we have identified a noteworthy pattern: certain meaningless special tokens 
                            (i.e., separators) contribute massively to attention scores compared to other 
                            semantically meaningful tokens. This insight has led us to hypothesize that 
                            the information of the segment between these special tokens can be condensed 
                            into these tokens without significant loss of information. Based on this hypothesis, 
                            we introduce SepLLM, a plug-and-play framework for inference acceleration 
                            by compressing these segments and dropping redundant tokens. Besides, 
                            we implement efficient kernels for training acceleration. The experimental 
                            results on training-free, training-from-scratch and post-training settings 
                            substantiate the effectiveness of SepLLM. Notably, SepLLM achieves a remarkable 
                            reduction of over 50\% in KV cache on the GSM8K-CoT benchmark, utilizing the Llama-3-8B backbone, 
                            with negligible compromise in performance. Additionally, in streaming settings, 
                            SepLLM is capable of delivering consistent and effective language modeling across 
                            up to 4 million tokens or even more. 
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <!-- End paper abstract -->

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop has-text-centered">
                <h2 class="title is-3">Overview</h2>
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <div class="item">
                            <!-- Your image here -->
                            <img style="width: 80%;" src="static/images/1.png" alt="metamath" />
                            <h2 class="subtitle">
                                Figure 1:  The training and inference paradigm of <i>SepLLM</i>.
                            </h2>
                        </div>

                        <br> 
                        <div class="item">
                            <!-- Your image here -->
                            <img style="width: 100%;" src="static/images/2.png" alt="metamath" />
                            <h2 class="subtitle">
                                Figure 2: Overall framework of the proposed <i>SepLLM</i> tailored for streaming applications.
                            </h2>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop has-text-centered">
                <h2 class="title is-3">Training-free</h2>
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <div class="item">
                            <div class="container is-max-desktop">
                                <div class="columns is-centered">
                                    <div class="column has-text-centered is-fifths-fifths">
                                        <!-- <h2 class="title is-3">Comprehensive Results</h2> -->
                                        <div class="content has-text-justified">
                            <!-- Your image here -->
                            <!-- <img style="width: 100%;" src="static/images/image.png" alt="metamath" /> -->
                            <div class="columns is-centered">
                            <table>
                                <tr>
                                  <th></th>
                                  <th>GSM8K-CoT</th>
                                  <th>r.KV(%)</th>
                                  <th>MMLU</th>
                                  <th>r.KV(%)</th>         
                                </tr>
                                <tr>
                                  <td>Vanilla</td>
                                  <td>77.3</td>
                                  <td>100.0</td>
                                  <td>65.7</td>
                                  <td>100.0</td>
                                </tr>
                                <tr>
                                    <td>StrmLLM (n=380)</td>
                                    <td>71.4</td>
                                    <td>47.5</td>
                                    <td>63.4</td>
                                    <td>52.5</td>
                                </tr>
                                <tr>
                                    <td>StrmLLM (n=256)</td>
                                    <td>68.6</td>
                                    <td>26.0</td>
                                    <td>62.1</td>
                                    <td>37.7</td>
                                </tr>
                                <tr>
                                    <td>SepLLM (n=256)</td>
                                    <td>77.2</td>
                                    <td>47.4</td>
                                    <td>64.7</td>
                                    <td>44.6</td>
                                </tr>
                              </table>
                            </div>
                            </div>
                            <h2 class="subtitle">
                                Table 1: Evaluation results and average running time KV cache usage for training-free experiments on GSM8K-CoT 8-shots and MMLU 5-shots.
                            </h2>
                            <div class="columns is-centered">
                                <video src="https://github.com/SepLLM/SepLLM.github.io/releases/download/SepLLM/1.mp4" controls="controls" muted="muted" class="d-block rounded-bottom-2 border-top width-fit" style="max-height:640px; min-height: 200px">
                                </video>
                            </div>
                            <h2 class="subtitle">
                                Video 1: The comparison of ppl and runing time between <i>SepLLM</i> and others.
                            </h2>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop has-text-centered">
                <h2 class="title is-3">Training from Scratch</h2>
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <div class="item">
                            <!-- Your image here -->
                            <img style="width: 100%;" src="static/images/train.png" alt="metamath" />
                            <h2 class="subtitle">
                                Figure 3: The loss comparison between vanilla Transformer and <i>SepLLM</i>.
                            </h2>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop has-text-centered">
                <h2 class="title is-3">Post-training</h2>
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <div class="item">
                            <!-- Your image here -->
                            <img style="width: 60%;" src="static/images/post.png" alt="metamath" />
                            <h2 class="subtitle">
                                Figure 4: Training loss curves for the post-training.
                            </h2>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered is-fifths-fifths">
                        <h2 class="title is-3">Comprehensive Results</h2>
                        <div class="content has-text-justified">
                            <table>
                                <thead>
                                    <tr>
                                        <th>Model</th>
                                        <th>#params</th>
                                        <th>GSM8K</th>
                                        <th>MATH</th>
                                    </tr>
                                    <tr>
                                    </tr>
                                </thead>
                                <tbody id="tabResults">
                                    <tr class="th">
                                        <td colspan="4" style="text-align: center; font-weight: bold;">
                                            Closed-source Model</td>
                                    </tr>
                                    <tr>
                                        <td>GPT-4</td>
                                        <td>-</td>
                                        <td>92.0</td>
                                        <td>42.5</td>
                                    </tr>
                                    <tr>
                                        <td>GPT-3.5-Turbo</td>
                                        <td>-</td>
                                        <td>80.8</td>
                                        <td>34.1</td>
                                    </tr>
                                    <tr>
                                        <td>PaLM</td>
                                        <td>8B</td>
                                        <td>4.1</td>
                                        <td>1.5</td>
                                    </tr>
                                    <tr>
                                        <td>PaLM</td>
                                        <td>62B</td>
                                        <td>33.0</td>
                                        <td>4.4</td>
                                    </tr>
                                    <tr>
                                        <td>PaLM</td>
                                        <td>540B</td>
                                        <td>56.5</td>
                                        <td>8.8</td>
                                    </tr>
                                    <tr>
                                        <td>PaLM-2</td>
                                        <td>540B</td>
                                        <td>80.7</td>
                                        <td>34.3</td>
                                    </tr>
                                    <tr>
                                        <td>Flan-PaLM 2</td>
                                        <td>540B</td>
                                        <td>84.7</td>
                                        <td>33.2</td>
                                    </tr>
                                    <tr>
                                        <td>Minerva</td>
                                        <td>8B</td>
                                        <td>16.2</td>
                                        <td>14.1</td>
                                    </tr>
                                    <tr>
                                        <td>Minerva</td>
                                        <td>62B</td>
                                        <td>52.4</td>
                                        <td>27.6</td>
                                    </tr>
                                    <tr>
                                        <td>Minerva</td>
                                        <td>540B</td>
                                        <td>58.8</td>
                                        <td>33.6</td>
                                    </tr>

                                    <tr class="th">
                                        <td colspan="4" style="text-align: center; font-weight: bold;">
                                            Open-source models (1-10B)</td>
                                    </tr>
                                    <tr>
                                        <td>LLaMA-1</td>
                                        <td>7B</td>
                                        <td>11.0</td>
                                        <td>2.9</td>
                                    </tr>
                                    <tr>
                                        <td>LLaMA-2</td>
                                        <td>7B</td>
                                        <td>14.6</td>
                                        <td>2.5</td>
                                    </tr>
                                    <tr>
                                        <td>MPT</td>
                                        <td>7B</td>
                                        <td>6.8</td>
                                        <td>3.0</td>
                                    </tr>
                                    <tr>
                                        <td>Falcon</td>
                                        <td>7B</td>
                                        <td>6.8</td>
                                        <td>2.3</td>
                                    </tr>
                                    <tr>
                                        <td>InternLM</td>
                                        <td>7B</td>
                                        <td>31.2</td>
                                        <td>-</td>
                                    </tr>
                                    <tr>
                                        <td>GPT-J</td>
                                        <td>6B</td>
                                        <td>34.9</td>
                                        <td>-</td>
                                    </tr>
                                    <tr>
                                        <td>ChatGLM 2</td>
                                        <td>6B</td>
                                        <td>32.4</td>
                                        <td>-</td>
                                    </tr>
                                    <tr>
                                        <td>Qwen</td>
                                        <td>7B</td>
                                        <td>51.6</td>
                                        <td>-</td>
                                    </tr>
                                    <tr>
                                        <td>Baichuan-2</td>
                                        <td>7B</td>
                                        <td>24.5</td>
                                        <td>5.6</td>
                                    </tr>
                                    <tr>
                                        <td>SFT</td>
                                        <td>7B</td>
                                        <td>41.6</td>
                                        <td>-</td>
                                    </tr>
                                    <tr>
                                        <td>RFT</td>
                                        <td>7B</td>
                                        <td>50.3</td>
                                        <td>-</td>
                                    </tr>
                                    <tr>
                                        <td>WizardMath</td>
                                        <td>7B</td>
                                        <td>54.9</td>
                                        <td>10.7</td>
                                    </tr>
                                    <tr>
                                        <td>MetaMath (<b>ours</b>)</td>
                                        <td>7B</td>
                                        <td><b>66.5</b></td>
                                        <td><b>19.8</b></td>
                                    </tr>
                                    <tr class="th">
                                        <td colspan="4" style="text-align: center; font-weight: bold;">
                                            Open-source models (11-50B)</td>
                                    </tr>
                                    <tr>
                                        <td>LLaMA-1</td>
                                        <td>13B</td>
                                        <td>17.8</td>
                                        <td>3.9</td>
                                    </tr>
                                    <tr>
                                        <td>LLaMA-1</td>
                                        <td>33B</td>
                                        <td>35.6</td>
                                        <td>7.1</td>
                                    </tr>
                                    <tr>
                                        <td>LLaMA-2</td>
                                        <td>13B</td>
                                        <td>28.7</td>
                                        <td>3.9</td>
                                    </tr>
                                    <tr>
                                        <td>LLaMA-2</td>
                                        <td>34B</td>
                                        <td>42.2</td>
                                        <td>6.2</td>
                                    </tr>
                                    <tr>
                                        <td>MPT</td>
                                        <td>30B</td>
                                        <td>15.2</td>
                                        <td>3.1</td>
                                    </tr>
                                    <tr>
                                        <td>Falcon</td>
                                        <td>40B</td>
                                        <td>19.6</td>
                                        <td>2.5</td>
                                    </tr>
                                    <tr>
                                        <td>GAL</td>
                                        <td>30B</td>
                                        <td>-</td>
                                        <td>12.7</td>
                                    </tr>
                                    <tr>
                                        <td>Vicuna</td>
                                        <td>13B</td>
                                        <td>27.6</td>
                                        <td>-</td>
                                    </tr>
                                    <tr>
                                        <td>Baichuan-2</td>
                                        <td>13B</td>
                                        <td>52.8</td>
                                        <td>10.1</td>
                                    </tr>
                                    <tr>
                                        <td>SFT</td>
                                        <td>13B</td>
                                        <td>50.0</td>
                                        <td>-</td>
                                    </tr>
                                    <tr>
                                        <td>RFT</td>
                                        <td>13B</td>
                                        <td>54.8</td>
                                        <td>-</td>
                                    </tr>
                                    <tr>
                                        <td>WizardMath</td>
                                        <td>13B</td>
                                        <td>63.9</td>
                                        <td>14.0</td>
                                    </tr>
                                    <tr>
                                        <td>MetaMath (<b>ours</b>)</td>
                                        <td>13B</td>
                                        <td><b>72.3</b></td>
                                        <td><b>22.4</b></td>
                                    </tr>

                                    <tr class="th">
                                        <td colspan="4" style="text-align: center; font-weight: bold;">
                                            Open-source models (50-70B)</td>
                                    </tr>
                                    <tr>
                                        <td>LLaMA-1</td>
                                        <td>65B</td>
                                        <td>50.9</td>
                                        <td>10.6</td>
                                    </tr>
                                    <tr>
                                        <td>LLaMA-2</td>
                                        <td>70B</td>
                                        <td>56.8</td>
                                        <td>13.5</td>
                                    </tr>
                                    <tr>
                                        <td>RFT</td>
                                        <td>70B</td>
                                        <td>64.8</td>
                                        <td>-</td>
                                    </tr>
                                    <tr>
                                        <td>WizardMath</td>
                                        <td>70B</td>
                                        <td>81.6</td>
                                        <td>22.7</td>
                                    </tr>
                                    <tr>
                                        <td>MetaMath (<b>ours</b>) <sup>‡</sup></td>
                                        <td>70B</td>
                                        <td><b>82.3</b></td>
                                        <td><b>26.6</b></td>
                                    </tr>
                        </div>
                        </tbody>
                        </table>
                    </div>
                    <h2 class="subtitle">Table 1: Comparison of testing accuracy to existing LLMs on GSM8K and MATH.
                        <sup>‡</sup>Due to the computing resource limitation, we finetune MetaMath-70B using QLoRA.
                    </h2>
                </div>
            </div>
        </div>
        </div>
    </section> -->


    <!--BibTex citation -->
    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@article{chen2024SepLLM,
  title={SepLLM: Accelerate Large Language Models by Compressing One Segment into One Token},
  author={Chen, Guoxuan and Shi, Han and Li, Jiawei and Ren, Xiaozhe and Chen, Yimeng and Jiang, Xin and Li, Zhenguo and Liu, Weiyang and Huang, Chao},
  journal={arXiv preprint arXiv:},
  year={2024}
}</code></pre>
        </div>
    </section>
    <!--End BibTex citation -->


    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p>
                            This page was built using the <a
                                href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
                            You are free to borrow the of this website, we just ask that you link back to this page in
                            the footer. <br> This website is licensed under a <a rel="license"
                                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                                Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>

                    </div>
                </div>
            </div>
        </div>
    </footer>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

</body>

</html>
